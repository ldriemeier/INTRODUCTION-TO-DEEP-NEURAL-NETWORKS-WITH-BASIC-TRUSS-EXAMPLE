{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MNpyc1WBC41"
      },
      "source": [
        "# AI application in Structural Engineering\n",
        "_Larissa Driemeier and Gabriel Lopes Rodrigues_\n",
        "\n",
        "\n",
        "This introductory notebook replicates the geometry of the structure analysed in the paper\n",
        "[*Background Information of Deep Learning for Structural Engineering*](https://www.researchgate.net/publication/318190131_Background_Information_of_Deep_Learning_for_Structural_Engineering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8799bui8E0s"
      },
      "source": [
        "## Structural Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS02BkT-5T63"
      },
      "source": [
        "### Geometry\n",
        "\n",
        "The figure below shows a beam like 2D truss with 10 bars. The length of the bars are fixed, however the cross section areas are obtained through a random uniform sampling between $0.6$ $cm^2$ and $225.8$ $cm^2$. In total, 500 different structures were generated.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1xOuJYBWiWGkq5l_Z_hcAjYak_hjG26l5)\n",
        "\n",
        "Then, the structure is loaded and analysed in the commercial FE software Abaqus. Since the dimensions in the structure are fixed, the input the set of areas, while all nodal displacements and also bar stresses are computed as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57lFpyyBoFtD"
      },
      "source": [
        "### Linear material model\n",
        "\n",
        "The material characteristics are generic values for Aluminum alloy 6061, as listed below.\n",
        "\n",
        "Property  | Value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    | Unity&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "---   | --- | ---\n",
        "Mass density $\\rho$ | $2.768\\times  10^{-9}$ | $ton/mm^2$\n",
        "Poisson $\\nu$ | $0.35$ | -\n",
        "Young's Modulus $E$| $68950$ | $MPa$\n",
        "Yield stress $\\sigma_{y0}$| $200$ | $MPa$\n",
        "\n",
        "The material undergoes elastic deformation until it reaches the elastic limit defined by the yield stress. After the elastic limit, the material exhibits plastic behavior,that is, the material deforms irreversibly and does not return to its original shape and size, even when the load is removed. Initially, only elastic behaviour is considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifh1T81H7wHt"
      },
      "source": [
        "## Machine Learning tool\n",
        "Virtually  no one develops its own code to implement and train a ANN since there are numerous development tools, already tested, that do most of this work and are widely used.\n",
        "\n",
        "The great advantage of using one of these tools comes from the fact that we only need to define the configuration (architecture) of the RNA, that is, to define how *forward propagation* is performed. When forward propagation is defined, the back propagation, which is in fact the most difficult part of coding an ANN, is automatically generated using symbolic manipulation.\n",
        "\n",
        "The most used deep-learning tools today are the following:\n",
        "- TensorFlow;\n",
        "- Keras;\n",
        "- Pytorch;\n",
        "- Caffe;\n",
        "- Theano;\n",
        "- MXNET;\n",
        "- CNTK;\n",
        "- Others.\n",
        "\n",
        "Almost all of these tools are freely available on the Internet.\n",
        "\n",
        "The Figure below shows the classification of these tools by users in the deep-learning area, which also provides an indication of the relative percentage of use of these tools.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1bTRu1eKniwD9nVB-i0F2LG5as--bFe6e)\n",
        "\n",
        "\n",
        "Google's TensorFlow and Facebook's PyTorch are both widely used machine learning and deep learning frameworks. TensorFlow, a symbolic math library used for machine learning and training neural networks, was open sourced in 2015 and backed by a huge community of machine learning experts.\n",
        "\n",
        "PyTorch, on the other hand, is a Python package released by Facebook in 2016 for training neural networks. It quickly gained popularity because developers found it easy to use unlike TensorFlow.\n",
        "\n",
        "Keras was developed by the MIT and is the most used deep learning framework among top-5 winning teams on Kaggle. It is, nowadays, TensorFlow's high-level API. Originally, Keras' default backend was Theano. When Google released TensorFlow, Keras started supporting TensorFlow as a backend, being the default since the release of Keras v1.1.0.\n",
        "\n",
        "Both TensorFlow and Keras usage grew together and, finally, the `tf.keras` submodule was introduced in TensorFlow v1.10.0, the first step in integrating Keras directly within the TensorFlow package itself.\n",
        "\n",
        "Keras actually consists of only a more user-friendly interface for other tools, so to use Keras we must also have TensorFlow, or Theano, or CNTK installed on the computer.\n",
        "\n",
        "Some Keras commands from TensorFlow are slightly different from the original Keras, so a program made for Keras must be modified to run with TensorFlow's Keras, but modifications are few and simple.\n",
        "\n",
        "Care must be taken with the numerous versions of TensorFlow and Keras, because a program made for an older version may not work with a newer version.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rLzO6oiTgpW"
      },
      "source": [
        "### Keras\n",
        "\n",
        "Keras is a tool for developing deep-learning ANNs, based on the Python language, which provides a simple and convenient way to build, train and test an ANN.\n",
        "\n",
        "The fundamental structure of the ANNs is the layer, which receives a tensor as input and generate another tensor as an output.\n",
        "\n",
        "There are several types of layers, each type being specific for a given tensor format and for a certain type of processing. So, for example:\n",
        "\n",
        "- Data in the form of vectors are stored in 2D tensors (1st axis: examples; 2nd axis: characteristics) and typically processed in densely connected layers, called *dense* layers;\n",
        "\n",
        "\n",
        "- Grayscale image data is stored in 3D tensors (1st axis: examples; 2nd axis: height; 3rd axis: width) and typically processed in *convolutive* layers;\n",
        "\n",
        "\n",
        "- Sequences of temporal data are stored in 3D tensors (1st axis: examples; 2nd axis: time; 3rd axis: characteristics) and typically processed in *recurring* layers, for example, LSTM or GRU layers.\n",
        "\n",
        "The layers can be seen as *blocks* that we use to build an ANN.  Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines.\n",
        "The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain format and will return output tensors of a certain format.\n",
        "\n",
        "In Keras there are two ways to define an ANN:\n",
        "\n",
        "- as *Sequential class* - configure models with a single sequence of layers, which is the most common type of ANNs;\n",
        "\n",
        "- as *Functional class* - configure models with sequences of cyclic or tree layers, allowing a totally arbitrary ANN configuration.\n",
        "\n",
        "\n",
        "We will start with the simplest way to create an RNA in Keras, which is the sequential model. Creating, training and testing an ANN with Keras is done in the following steps:\n",
        "- Definition of training and test data;\n",
        "- ANN configuration, which consists of defining the layers to map the inputs to the desired outputs;\n",
        "- Compilation of the ANN, which also includes configuring the training process by choosing the cost function, the optimizer and the metric to evaluate performance;\n",
        "- ANN training;\n",
        "- ANN performance evaluation.\n",
        "\n",
        "The Keras documentation provides details on its use. This documentation can be seen at the [link](https://keras.io/).\n",
        "\n",
        "The Keras manual for TensorFlow is available at the [link](https://www.tensorflow.org/api_docs/python/tf/keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-ytuiSdBC43"
      },
      "source": [
        "### Libraries\n",
        "Throughout this notebook the new version 2.12 of Tersorflow was used, with built-in keras support, which has been recently released to the public.\n",
        "To install it, just follow the instructions in [the official website](https://www.tensorflow.org/install), to guarantee that the right version is installed.\n",
        "\n",
        "The rest of the libraries used were simply installed using pip, the default Python tool for installing packages. These include:\n",
        "\n",
        "- NumPy: library for dealing with large matrices and also providing optimized functions for these data structures\n",
        "\n",
        "- Pandas: used to visualize the data and to work with the dataset.\n",
        "\n",
        "- matplotlib: used to generate plots from the models.\n",
        "\n",
        "- sklearn (also known as scikit-learn): used because of the many useful functions and utilities it provides for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMBfXDXABC43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3f7cd11d-b271-4d97-e453-973e47fb2593"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ATF9uyBC47"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMEnmW28BC49"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHa54RD05ude"
      },
      "source": [
        "## Data Loading and Manipulation\n",
        "\n",
        "Uploading four files:\n",
        "1. the dataset containing the areas, `areas.csv`;\n",
        "2. displacements and reaction force along the time, `FinalResult.csv`;\n",
        "\n",
        "If you prefer generate new data, we suggest to use the student version of the software [Abaqus](https://edu.3ds.com/en/software/abaqus-student-edition). The following files are available in the same [link](https://edisciplinas.usp.br/course/view.php?id=82602#section-3):\n",
        " 1. To generate random areas `AreaGeneration.ipynb`;\n",
        " 2. Script to run in Abaqus to generate data `10_BarStructure.py`;\n",
        " 3. Basic geometry to be called by the script mentioned in item 2 `Job-10bar.inp`;\n",
        " 4. Copy the file `extracted_data_DATA_HOUR.csv` as `FinalResult.csv` to upload.\n",
        "\n",
        " **Important**\n",
        "\n",
        "The script in item 02 automatically generates the bar geometry in Abaqus. If you want to build up a geometry - at least once - with Abaqus, Prof Marcilio Alves kindly prepared a tutorial that can be accessed through the [link](https://www.youtube.com/channel/UCEDn-UheEHKLfOKJKmSKzJw).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or0FZJoDncwC"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ahz20vBC5C"
      },
      "source": [
        "As shown below, there are 10 different areas, which will be the inputs, and various other measurements, which might be used as outputs of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJAFdYK8BC5A"
      },
      "source": [
        "df = pd.read_csv('FinalResult.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpm5RpQdBC5D"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2P15QlKBC5F"
      },
      "source": [
        "# To show all the columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WGv5XThBC5H"
      },
      "source": [
        "### Splitting dataset\n",
        "\n",
        "The whole dataset will be split into training and test sets and organized in tensors.\n",
        "The training set will be used to train the model and the test set to verify its performance.\n",
        "\n",
        "The way the data is organized depends on the data type. Keras expects the first axis of the data, both in and out of the ANN, to be the number of examples $m$.\n",
        "\n",
        "For example, in our problem where the input data for each example is a vector with $ n_x = 10$ areas, the output is a vector with $n_y = 2$ and there are $m = 520$ examples, so the input and output tensors of the ANNs expected by Keras are as follows:\n",
        "\n",
        "- Size of the input tensor $(m, n_x)$;\n",
        "- Size of the output tensor $(m,n_y)$.\n",
        "\n",
        "When divided into train $(80\\%)$ and test $(20\\%)$, we expect the dimensions $(416,10)$, $(104,10)$ for train and test input and $(104,10)$, $(104,2)$ for train and test output, respectively.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyOEcTmLBC5I"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jardg69qBC5K"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtoamKYSBC5O"
      },
      "source": [
        "#### Defining the training values and the expected outputs\n",
        "\n",
        "The input for the NN is a vector with all 10 areas that compound the geometry of the structure. Remember that we mantain all other parameters, such as material properties and dimensions, fixed.\n",
        "\n",
        "Formato esperado dos dados keras o Keras espear que o primeiro eixo dos dados, tanto de entrada como de saída da RNA seja p número de exemplos. Dimensão do tensor de entrada (m,nx). Dimensão do tensor de saída (m,ny).\n",
        "\n",
        "As output for the NN, let's generate an array of displacements (\\[*d4*\\]). This values correspond to the vertical displacement of the rightmost node of the structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5vD_siBC5P"
      },
      "source": [
        "x_train = train.loc[:,'area1':'area10'].values\n",
        "y_train = train[['d4']].values\n",
        "x_val = test.loc[:,'area1':'area10'].values\n",
        "y_val = test[['d4']].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FAeW_EoccsU"
      },
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print(x_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G27Ww8pvUC-"
      },
      "source": [
        "### FEA results\n",
        "\n",
        "Read the results from FEA, where `d2` and `d4` are the displacements at the rightmost nodes of the structure to see the variation our future NN has to learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JW7edpTvEjK"
      },
      "source": [
        "disp4 = df[['d4']].values\n",
        "xAxis = [i + 1.0 for i, _ in enumerate(disp4)]\n",
        "plt.scatter(xAxis,disp4,color='darkslateblue',s=8, label =r'$d_4$')\n",
        "plt.title('Displacement at the end of the truss structure')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZHeVHAdBC5R"
      },
      "source": [
        "### Normalizing Dataset\n",
        "\n",
        "Most of times different features in the data might be have varying magnitudes. For example, a dataset containing two resources, displacement($x_1$), which ranges from 0-1) and stresses ($x_2$), about 100-1000 times greater than displacement. So, these two features are at very different ranges with high values dominating those with small values. The reason is that many of the machine learning algorithms use euclidean distance between data point in their computation. In this case, machine learning model treats those with small values as if they don't exist.\n",
        "\n",
        "To ensure that this is not the case, we need to scale our resources in the same range, that is, within the range of -3 and 3 or -1/3 and 1/3.\n",
        "\n",
        "The scikit-learn preprocessing module has excellent api and documentation on feature scaling [here](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAXrZqmj-31"
      },
      "source": [
        "We'll normalize our dataset using the scikit-learn object [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
        "\n",
        "Good practice usage with the MinMaxScaler and other scaling techniques is as follows:\n",
        "\n",
        "* __Fit the scaler using available training data__ For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the `fit()` function.\n",
        "* __Apply the scale to training data__ This means you can use the normalized data to train your model. This is done by calling the `transform()` function.\n",
        "* __Apply the scale to test data__ This means you can use the normalized data to test your model. This is done by calling the `transform()` function.\n",
        "* __Apply the scale to data going forward__ This means you can prepare new data in the future on which you want to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV31l7fcrwEC"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scaling the input data using the MinMaxScaler from scikit-learn\n",
        "scaler_x = MinMaxScaler().fit(x_train)\n",
        "x_train_sca = scaler_x.transform(x_train)\n",
        "x_val_sca = scaler_x.transform(x_val)\n",
        "\n",
        "# Normalizing the output data using the normalizer from scikit-learn\n",
        "normalizer_y = MinMaxScaler(feature_range = (-1.,0.)).fit(y_train)#StandardScaler,MaxAbsScaler\n",
        "y_train_sca = normalizer_y.transform(y_train)\n",
        "y_val_sca = normalizer_y.transform(y_val)\n",
        "\n",
        "\n",
        "#  Min and Max in input\n",
        "min_x_train = np.min(x_train_sca)\n",
        "min_x_val = np.min(x_val_sca)\n",
        "max_x_train = np.max(x_train_sca)\n",
        "max_x_val = np.max(x_val_sca)\n",
        "\n",
        "# Mean and Standard Deviation in Output\n",
        "min_y_train = np.min(y_train_sca)#mean\n",
        "min_y_val = np.min(y_val_sca)\n",
        "max_y_train = np.max(y_train_sca)#std\n",
        "max_y_val = np.max(y_val_sca)\n",
        "\n",
        "\n",
        "print(f'For the input training set, the min is {min_x_train} and the max is {max_x_train}')\n",
        "print(f'For the input validation set, the min is {min_x_val} and the max is {max_x_val}')\n",
        "print(f'For the output train set, the min is {min_y_train} and the max is {max_y_train}')\n",
        "print(f'For the output validation set, the min is {min_y_val} and the max is {max_y_val}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEQDFdR0BC5U"
      },
      "source": [
        "As can be seen, the training set is always between 0 and 1. Since the dataset is normalized with respect to the training set, for the val data, the limits are slightly different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlNNpHuqiQ6Q"
      },
      "source": [
        "## First ANN model\n",
        "\n",
        "This model is a Neural Network with architecture (10-20-1), sigmoid activation function and Stochastic Gradient Descendent (SGD) as the optimizer. It is exactly the first model described in the paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1h4_elMwSxK"
      },
      "source": [
        "### Configuration\n",
        "\n",
        "The architecture (10-20-1) our network consists of one input layer with 10 features, followed by a sequence of two Dense layers, which are fully connected neural layers. The first hidden layer has 20 neurons, and the second (and last) layer has 1 neuron.\n",
        "\n",
        "See that the last activation function is not defined - default in Keras is set to `None`. That means, it is linear $g(z)=z$. That means that by default it is a linear activation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7EL_kXTdhqv"
      },
      "source": [
        "We will  create the sequencial ANN in two ways:\n",
        "1. Passing a list of layer instances to the Keras constructor;\n",
        "2. In steps using commands to add one layer at a time, using `add()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBL_l784eDnN"
      },
      "source": [
        "from keras import models\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "##First definition\n",
        "\n",
        "model = models.Sequential([\n",
        "    Dense(20, input_shape=(10,)),\n",
        "    Activation('sigmoid'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUj6C4o7fcIw"
      },
      "source": [
        "This command defines an RNA of an intermediate layer and an output layer with the following characteristics:\n",
        "   * Input data for each training example is a 1-D vector (10);\n",
        "   * It is observed that the dimension of the second axis of the input tensor is not included in the ʻinput_shape` argument, because at that moment the number of examples that will be used in training is unknown;\n",
        "   * Be careful - although it seems that in the `input_shape` argument the second axis is the number of examples, Keras expects the first axis of the input tensor to be the number of examples;\n",
        "   * The hidden layer is of the dense type (fully connected), it has 20 neurons and its activation function is sigmoid;\n",
        "   * The output layer is dense (fully connected), has one neuron and its activation function is linear.\n",
        "\n",
        "The name used for this ANN was `model`, but any other name could be given.\n",
        "\n",
        "The `summary ()` method presents a summary of the main characteristics of the network.\n",
        "\n",
        "As you saw in the previous code, the architecture of the ANN is presented in a table, the content of which is as follows:\n",
        "   * First column provides the types of network layers and numbers the layers;\n",
        "   * Second column shows the number of neurons in the layer;\n",
        "   * Third column shows the number of layer parameters.\n",
        "\n",
        "And the valuer $220???$ Where did it come from?\n",
        "\n",
        "The number of parameters is the sum of those in weight matrix $ \\mathbf W$, dimension *number of neurons in the layer* per *number of inputs*, and the bias vector $\\mathbf b$, dimension *number of biases*:\n",
        "$$\n",
        "20\\times 10 + 20 = 220\n",
        "$$\n",
        "\n",
        "  It is important to visualize the RNA architecture because any error messages in its compilation or training, reference the layer by its number, which can be obtained by the `summary ()` method.\n",
        "\n",
        "Another way to visualize an RNA in Keras is to graph it using the `plot_model` function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYp5KoQBmckX"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "import pydot\n",
        "plot_model(model, to_file = '/content/model.png', show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npWzQbmUisbm"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "##Second definition\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(20, activation='sigmoid', input_shape=(10,)))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGli1m7ugs2a"
      },
      "source": [
        "In this case, instead of importing only the sequential model structure, the first command imports all types of models from Keras. The second command imports all types of layers from Keras, instead of only dense layers, as previously done.\n",
        "\n",
        "The third command creates the RNA instance using a sequential model.\n",
        "\n",
        "The fourth command adds the first layer of RNA of the dense type, with 20 neurons, with Sigmoid activation function, whose input is a line vector of dimension (10).\n",
        "\n",
        "The fifth command adds a second dense type layer, with 1 neuron and none activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-uwG07qhVWC"
      },
      "source": [
        "#### Dimensions of the input data\n",
        "\n",
        "The ANN needs to know the dimensions of the input data, for this reason the first layer in a sequential model needs to receive this information.\n",
        "\n",
        "It is observed that only the first layer needs this information, since Keras automatically infers the dimensions of the input data of the other layers of the ANN using the information of the number of neuron of each layer.\n",
        "\n",
        "There are several ways to define the size of the ANN input data:\n",
        "\n",
        "- Passing the `input_shape` argument to the first layer. This argument is a tuple of integers or simply `None`, where` None` indicates that any positive integer can be expected;\n",
        "\n",
        "- In the `input_shape` argument the number of examples is not included and Keras automatically infers this number from the input data provided;\n",
        "\n",
        "- Some 2D layers, such as dense layers, support the specification of the input data also via the `input_dim` argument.\n",
        "\n",
        "As an example, the following commands are equivalent:\n",
        "\n",
        "```\n",
        "ann = models.Sequential ()\n",
        "ann.add (Dense (20, input_shape = (10,)))\n",
        "```\n",
        "or\n",
        "```\n",
        "ann = models.Sequential ()\n",
        "ann.add (Dense (20, input_dim = 10))\n",
        "```\n",
        "\n",
        "If you need to specify a fixed number of examples, you can pass the `batch_size` argument to the input layer. Thus, for example, if `batch_size = 32` and `input_shape = (6, 8)` are used for the first layer, a dimension tensor (32, 6, 8) will be expected as input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCe7YUIDkfPS"
      },
      "source": [
        "The development of an ANN requires many iterations to obtain a desirable result, so to avoid executing the same configuration commands over and over again, which can be long depending on the size of the ANN, you can create a function to configure the RNA. For this we have, for example, the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brtoNRr2lWC0"
      },
      "source": [
        "def build_model(data_shape=(10,)):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(units=20, activation='sigmoid', input_shape=data_shape))\n",
        "    model.add(layers.Dense(units=1))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyauyd3BQw5T"
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5OdcHoHlr2V"
      },
      "source": [
        "In this case the argument `data_shape` represents the dimension of the input data without considering the number of examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkGkuWT-wkaB"
      },
      "source": [
        "### Compilation\n",
        "\n",
        "The generation of the ANN is performed in the compilation stage, where the loss function, the training method and the metrics for the ANN evaluation are defined and configurated:\n",
        "\n",
        "+ The loss function `mean_squared_error` — How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.  \n",
        "+ The optimizer `sgd` — The mechanism through which the network will update itself based on the data it sees and its loss function.\n",
        "+ Metrics to monitor during training and testing `mean_absolute_error`, `mean_absolute_percentage_error`.\n",
        "\n",
        "Keras uses the principle of making things simple, but at the same time it allows the user to control whatever is needed. If we want, we can configurate completely the optimizer.  See more details at the [link](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) or [here](https://ruder.io/optimizing-gradient-descent/).\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "Mean squared error $E$ is calculated as the average of the squared differences between the predicted $\\hat{\\mathbf{y}}^{(i)}$ and target values ${\\mathbf{y}}^{(i)}$,\n",
        "$$\n",
        "E(\\hat{\\mathbf{y}}^{(i)},{\\mathbf{y}}^{(i)})=\\sum\\limits_{j=1}^{n_y} \\left(\\hat{y}^{(i)}_j -{y}^{(i)}_j\\right)^2= \\left\\|\\hat{\\mathbf{y}}^{(i)} - {\\mathbf{y}}^{(i)} \\right\\|_2^2\n",
        "$$\n",
        "for the data $i$, $i=1,...m$.\n",
        "Then, the loss function $J$\n",
        "$$\n",
        "J\\left(\\mathbf{W},\\mathbf{B}\\right)={1\\over m}\\sum\\limits_{i=1}^{m}E(\\hat{\\mathbf{y}}^{(i)},{\\mathbf{y}}^{(i)}) = {1\\over m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{n_y} \\left(\\hat{y}^{(i)}_j -{y}^{(i)}_j\\right)^2 = {1\\over m} \\sum\\limits_{i=1}^{m}\\left\\|\\hat{\\mathbf{y}}^{(i)} - {\\mathbf{y}}^{(i)} \\right\\|_2^2\n",
        "$$\n",
        "depends on the weights $\\mathbf{W}$ and bias $\\mathbf{b}$ parameters.\n",
        "The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0.\n",
        "\n",
        "The squaring means that larger mistakes result in more error than smaller mistakes, that is, the model is punished for making larger mistakes.\n",
        "\n",
        "The mean squared error loss function can be used in Keras by specifying ‘mse‘ or `mean_squared_error` as the loss function when compiling the model.\n",
        "\n",
        "### Optimizer\n",
        "\n",
        "SGD is the same as gradient descent, except that it is used to split the data into batches. The parameter is called *mini-batch size*.\n",
        "\n",
        "Faster optimizers are available in the literature to speed up the training step. We will apply the SGD + Momentum (known as SGD), but, be aware that are other  popular Optimizer approaches such as Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam, and Nadam optimization.\n",
        "\n",
        "The best optimizer, according to the literature, is Adam. See more about this optimizer in [link](https://www.aiplusinfo.com/blog/what-is-the-adam-optimizer-and-how-is-it-used-in-machine-learning/).\n",
        "\n",
        "The SGD optimizer used here has a learning rate of 0.001 and momentum of 0.9.\n",
        "\n",
        "### Metrics\n",
        "\n",
        "A metric or  Key Performance Indicator (KPI) is a function that is used to judge the performance of your model. The most commonly used are defined below.\n",
        "\n",
        "**MAE**\n",
        "\n",
        "The Mean Absolute Error (`mean_absolute_error`,`MAE`, `mae`) computes the mean absolute error between the labels and predictions\n",
        "$$\n",
        "MAE = \\frac{1}{n} \\sum_1^n |y^{(i)} - \\hat{y}^{(i)}|\n",
        "$$\n",
        "wher $n$ is the length of the validation dataset.\n",
        "\n",
        "**MAPE**\n",
        "\n",
        "The Mean Absolute Percentage Error (`mean_absolute_percentage_error`, `MAPE`, `mape`) is one of  which is\n",
        "$$\n",
        "MAPE = \\frac{100}{n} \\sum_i^n \\frac{y^{(i)} - \\hat{y}^{(i)}}{y^{(i)}}\n",
        "$$\n",
        "\n",
        "Similar to MAE, but normalized by true observation. Downside is when true observation value $\\hat{y}^{(i)}$ is zero or near to zero, this metric will be problematic.\n",
        "\n",
        "**MSE**\n",
        "\n",
        "Mean squared error (`mean_squared_error`, `MSE` or `mse`) is a quadratic scoring rule that also measures the average magnitude of the error. It’s the average of squared differences between prediction and actual observation,\n",
        "$$\n",
        "MSE =\\frac{1}{n} \\sum_i^n (y^{(i)} -\\hat{y}^{(i)})^2\n",
        "$$\n",
        "\n",
        "MSE is like a combination measurement of bias and variance of your prediction, i.e., $MSE = Bias^2 + Var$.\n",
        "\n",
        "MAE and MSE are two of the most common metrics used to measure accuracy for continuous variables. They express average model prediction error in units of the variable of interest, can range from $0$ to $+\\infty$ and are indifferent to the direction of errors. They are negatively-oriented scores, which means lower values are better.\n",
        "\n",
        "Taking the average of the squared errors has some interesting implications for MSE. Since the errors are squared, the RMSE gives a relatively high weight to large errors. This means the MSE should be more useful when large errors are particularly undesirable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwlBi4P-wnep"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4dj_21ji8DI"
      },
      "source": [
        "The code below is a callback to create a loss history for the validation set. It evaluates the model after each epoch and appends the result into an array. It is used to generate a plot of the model loss for both the training and the validation sets.\n",
        "\n",
        "However, it is also very slow and makes training last for more than half an hour with large epochs, which is why the code to call it is normally commented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKATE-Cti6r_"
      },
      "source": [
        "class TestLossHistory(keras.callbacks.Callback):\n",
        "    def __init__(self, x_test, y_test):\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.i = 0\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        #print(f\"logs: {logs}\")\n",
        "        self.losses.append(self.model.evaluate(self.x_test, self.y_test))\n",
        "    def on_train_end(self, logs={}):\n",
        "        self.losses = np.array(self.losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_SwzFsTNWqs"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8-AUpRmkAQ-"
      },
      "source": [
        "#### Gradient descent\n",
        "\n",
        "The Gradient Descent (GD) method is the basic *motor* of artificial neural networks.\n",
        "\n",
        "As discussed in class, GD is an iterative method, following the steps:\n",
        "\n",
        "* Initialization of the ANN parameters: assign an initial value to the parameters weights ($\\mathbf W$), and bias ($\\mathbf b$);\n",
        "* Execution of the ANN for all examples of the training data set, so that given the inputs, the outputs predicted by the ANN are calculated;\n",
        "* Calculation of the loss function for all training examples, through the sum of the error function;\n",
        "* Calculation of the gradient of the cost function in relation to all parameters of the ANN;\n",
        "* Updating of ANN parameters in the opposite direction to the gradient in order to reduce the value of the loss function.\n",
        "\n",
        "The steps 2 to 5 are iterative. Therefore, training a deep neural network can be an extremely time-consuming task especially with complex problems.\n",
        "\n",
        "In the algorithm presented before, updating the ANN parameters is done only after calculating the gradient of the cost function for all training examples and this can be a big problem, or even unfeasible, if we have a large number of training examples, for example, something in the order of 100 thousand or 1 million, which is not uncommon.\n",
        "\n",
        "To avoid computer memory problems, the optimization process can be changed in order to update the ANN parameters after processing only a few examples (a *batch*) of the training dataset. In general, this process is called in the literature as Stochastic Gradient Descent (SGD) because the training data is randomly divided into smaller sets.\n",
        "\n",
        "There literature is a bit confused about nomenclature of the other versions of the GD. As far as the size of the dataset used to update parameters is concern,  the options are,\n",
        "* Batch Gradient Descent (BGD) or simply GD - running on a full dataset. Gradient is more general, but intractable for huge datasets;\n",
        "* Stochastic Gradient Descent (SGD) - picking a random instance at each step.  Gradient can be noisy;\n",
        "* Mini-batch Gradient Descent (MBGD) or also Stochastic Gradient Descent (SGD) - running on random subsets of the data with dimension `bath_size` - looking for a balance between running the full data set or only one before update the parameters. Not very noisy and computationally tractable, that means, best of both worlds.\n",
        "\n",
        "As usually, there is no free lunch and all have pros and cons. Batch Gradient Descent can reach the global minimum at a terribly slow pace, specially with huge problems since in modern day architectures, the number of parameters may be in billions. Mini-batch Gradient Descent gets to the global minimum faster than BGD but it is easier to get stuck in the local minimum, and SGD is usually harder to get to the global minimum compared to the other two.\n",
        "\n",
        "We'll train our NN using MBGD and BGD, in order to compare effectiveness.\n",
        "\n",
        "In Keras `batch_size` refers to the batch size in MBGD. The default in keras is a MBGD with `batch_size=32`. If you want to run a GD, you need to set the `batch_size` to the number of training samples.\n",
        "\n",
        "Below, train and test are performed with mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "888J2FRRsf11"
      },
      "source": [
        "The training of an ANN is carried out with the `fit()` method. For example, train an ANN using 10 epochs, the command used is as follows:\n",
        "\n",
        "\n",
        "```\n",
        "model.fit (x_train, y_train, epochs = 10, verbose = 2)\n",
        "```\n",
        "The fit method performs the training of the ANN with the training examples composed of the input data, `x_train`, and the output data, `y_train`.\n",
        "`epochs = 10` means that 10 training seasons are used and `verbose = 2` means that after each epoch the values of the loss function and the metrics are presented.\n",
        "\n",
        "There are several arguments for the fit method, which you can explore, as your knowledge of the subject grows, and your needs...\n",
        "\n",
        "All options for the fit method can be seen in detail in the Keras documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHRZYqrmnwc"
      },
      "source": [
        "history_with_minibatch = model.fit(x_train_sca, y_train_sca, epochs=500, batch_size=32, verbose=2)\n",
        "\n",
        "# To use the test loss history, comment the lines above and uncomment the lines below\n",
        "#test_history_with_minibatch = TestLossHistory(x_val_sca, y_val_sca)\n",
        "#history_with_minibatch = model.fit(x_train_sca, y_train_sca, epochs=10000, batch_size=32,\n",
        "#                                   callbacks=[test_history_with_minibatch])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO0FiEfit3cR"
      },
      "source": [
        "#### Saving the training process\n",
        "\n",
        "If the training process is saved, it is possible to graph the loss function, allowing a more detailed analysis of the process. For this we use:\n",
        "\n",
        "\n",
        "```\n",
        "history_MODEL = model.fit (x_train, y_train, epochs = 1000)\n",
        "```\n",
        "In this training command the values of the cost function and the metric according to the seasons are saved in the `history_MODEL` object.\n",
        "\n",
        "The `history_MODEL` object contains a dictionary with the values of the loss function and metrics for each epoch, which can be accessed using the following comment:\n",
        "```\n",
        "history_dict = history_MODEL.history\n",
        "history_dict.keys ()\n",
        "```\n",
        "The `history_dict.keys` command displays the dictionary contents saved during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLhZwedGmrCi"
      },
      "source": [
        "history_with_minibatch.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tePZj3exmtJi"
      },
      "source": [
        "plt.plot(history_with_minibatch.history['loss'],linewidth=2.0)\n",
        "plt.title('Training Mean Squared Error (MSE)\\n with architecture (10-20-1) using mini-batch', fontsize=12)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Loss')\n",
        "#plt.plot(test_history_with_minibatch.losses.T[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNYOJKRCnMk-"
      },
      "source": [
        "The same training is performed using BGD, that means, the size of the batch is the length of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Icyvw8nLGp"
      },
      "source": [
        "# Redefining the model\n",
        "model2 = build_model()\n",
        "\n",
        "model2.compile(optimizer=sgd,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InVA8JFhnQ6J"
      },
      "source": [
        "history_without_minibatch = model2.fit(x_train_sca, y_train_sca, epochs=500, batch_size=x_train.shape[0], verbose = 0)\n",
        "\n",
        "\n",
        "# To use the test loss history, comment the lines above and uncomment the lines below\n",
        "#test_history_without_minibatch = TestLossHistory(x_test, y_test)\n",
        "#history_without_minibatch = model2.fit(x_train_norm, y_train, epochs=10000, batch_size=x_train.shape[0],\n",
        "#                                       callbacks=[test_history_without_minibatch])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQArKHdSF5G7"
      },
      "source": [
        "### ANN performance\n",
        "\n",
        "After training the ANN it is important to evaluate its performance with new data, that is, not used in the training.\n",
        "\n",
        "The evaluation of the ANN with the test data set can be done using the `evaluate` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJiTro25nYVS"
      },
      "source": [
        "model2_metric_train = model2.evaluate(x_train_sca, y_train_sca)\n",
        "print('Training performance')\n",
        "print(model2_metric_train)\n",
        "model2_metric_test = model2.evaluate(x_val_sca, y_val_sca)\n",
        "print('Test performance')\n",
        "print(model2_metric_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9DGbxa8na-9"
      },
      "source": [
        "plt.plot(history_with_minibatch.history['loss'], label='With minibatch',linewidth=1.5)\n",
        "plt.title('Training Mean Squared Error (MSE)\\nwith neural network architecture (10-20-1)', fontsize=12)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(history_without_minibatch.history['loss'], label='Without minibatch',linewidth=1.5)\n",
        "plt.legend()\n",
        "plt.xlim([-10, 200])\n",
        "#plt.plot(test_history_without_minibatch.losses.T[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eykmk_nXne_g"
      },
      "source": [
        "As can be seen above, with the use of minibatches, the training converged much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgR5sU6ROCof"
      },
      "source": [
        "The ANN can also be evaluated by calculating the expected outputs from the test set examples using the `predict` method, as follows:\n",
        "\n",
        "```\n",
        "y_prev = model.predict(x_test)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfQ3g7qLjWrC"
      },
      "source": [
        "Initially, let's define a function to plot target in blue and prediction in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kthdvnOUdQk"
      },
      "source": [
        "pred_sca_train = model.predict(x_train_sca)\n",
        "pred_sca_val = model.predict(x_val_sca)\n",
        "print(pred_sca_val.shape,pred_sca_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFBuUbDCUunI"
      },
      "source": [
        "y_new_train = normalizer_y.inverse_transform(pred_sca_train)\n",
        "y_new_val = normalizer_y.inverse_transform(pred_sca_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJBJTUglOBlB"
      },
      "source": [
        "# Graph of actual and predicted classes\n",
        "def Target_vs_Predic(y_test,y_pred):\n",
        "  plt.figure(figsize=(16, 6))\n",
        "  plt.plot(y_test[:,0], 'o', color = 'darkslateblue', label='Target')\n",
        "  plt.plot(y_pred[:,0], 'o', color = 'crimson', label='ANN prediction')\n",
        "  plt.title('Target vs prediction of the ANN')\n",
        "  plt.xlabel('Example')\n",
        "  plt.ylabel(r'$d_{4} [mm]$')\n",
        "  plt.legend()\n",
        "  plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxXKu_X4OzYr"
      },
      "source": [
        "Target_vs_Predic(y_val,y_new_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRdi6FJ2QeRP"
      },
      "source": [
        " ![](https://drive.google.com/uc?export=view&id=1M4Yho73GUFuFCr0CqNOrv5UhvV5r60Jg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2-kQZw4nwgS"
      },
      "source": [
        "## Changing number of neurons in the hidden layer\n",
        "\n",
        "Up to this point, the number os neurons in the hidden layer was chosen rather arbitrarily as 20. To experiment, with this, the architectures of (10−10−1), (10−20−1), (10−30−1), (10−40−1), and (10−50−1) will be considered, keeping the rest as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33bvqFGNneeG"
      },
      "source": [
        "# Creates a model with the specific number of neurons num_neurons and specific activation g\n",
        "def make_model(num_neurons=20, g = 'sigmoid'):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(units=num_neurons, activation=g, input_shape=(10,)))\n",
        "  model.add(layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer=sgd,\n",
        "                loss='mean_squared_error',\n",
        "                metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McBJqYXF3lDa"
      },
      "source": [
        "model_10_neurons = make_model(num_neurons=10)\n",
        "model_20_neurons = make_model(num_neurons=20)\n",
        "model_30_neurons = make_model(num_neurons=30)\n",
        "model_40_neurons = make_model(num_neurons=40)\n",
        "model_50_neurons = make_model(num_neurons=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFN--Tlin3zu"
      },
      "source": [
        "# Training the models - output will be suppressed\n",
        "print('10 neurons')\n",
        "hist_10_neurons = model_10_neurons.fit(x_train_sca, y_train_sca, epochs=500, verbose=0)\n",
        "print('20 neurons')\n",
        "hist_20_neurons = model_20_neurons.fit(x_train_sca, y_train_sca, epochs=500, verbose=0)\n",
        "print('30 neurons')\n",
        "hist_30_neurons = model_30_neurons.fit(x_train_sca, y_train_sca, epochs=500, verbose=0)\n",
        "print('40 neurons')\n",
        "hist_40_neurons = model_40_neurons.fit(x_train_sca, y_train_sca, epochs=500, verbose=0)\n",
        "print('50 neurons')\n",
        "hist_50_neurons = model_50_neurons.fit(x_train_sca, y_train_sca, epochs=500, verbose=0)\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed8Cl5j7n50I"
      },
      "source": [
        "plt.title('MSE for the models with\\n varying number of neurons in hidden layers', fontsize=12)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(hist_10_neurons.history['loss'], label='10',linewidth=1.0)\n",
        "plt.plot(hist_20_neurons.history['loss'], label='20',linewidth=1.0)\n",
        "plt.plot(hist_30_neurons.history['loss'], label='30',linewidth=1.0)\n",
        "plt.plot(hist_40_neurons.history['loss'], label='40',linewidth=1.0)\n",
        "plt.plot(hist_50_neurons.history['loss'], label='50',linewidth=1.0)\n",
        "plt.xlim([0,20])\n",
        "#plt.ylim([0,2500])\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sAmn-hyn8vQ"
      },
      "source": [
        "From the plot above, it can be seen that the learning gets faster as the number of neurons increases. However, from 40 to 50 neurons the difference is less significative than from 10 to 20 neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAMGeA0be3gG"
      },
      "source": [
        "# Graph of actual and predicted classes\n",
        "def Target_vs_Predic2(y_test,y_pred1,label1,y_pred2,label2):\n",
        "  plt.figure(figsize=(16, 6))\n",
        "  plt.plot(y_test[:,0], 'o', color = 'darkslateblue', label='Target')\n",
        "  plt.plot(y_pred1[:,0], 'o', color = 'crimson', label=label1)\n",
        "  plt.plot(y_pred2[:,0], 'o', color = 'gold', label=label2)\n",
        "  plt.title('Target vs prediction of the ANN')\n",
        "  plt.xlabel('Example')\n",
        "  plt.ylabel(r'$d_{4} [mm]$')\n",
        "  plt.legend()\n",
        "  plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgZXmwA2SJi-"
      },
      "source": [
        "pred_sca_val = model_10_neurons.predict(x_val_sca)\n",
        "y_new_val1 = normalizer_y.inverse_transform(pred_sca_val)\n",
        "\n",
        "pred_sca_val = model_50_neurons.predict(x_val_sca)\n",
        "y_new_val2 = normalizer_y.inverse_transform(pred_sca_val)\n",
        "\n",
        "Target_vs_Predic2(y_val,y_new_val1,'10 neurons', y_new_val2, '50 neurons')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7TtDgxQpVIu"
      },
      "source": [
        "## Activation functions\n",
        "\n",
        "Until now, the sigmoid or logistic function,\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
        "was used to create activate the neurons.\n",
        "\n",
        "The purpose of the activation function is to introduce non-linearity into the output of a neuron. See the figure below for the most commonly used activation functions.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1iZPDL1JuFLeH9uOc-SfbTfBWBnXwamRM)\n",
        "\n",
        "This non-linearity is one of the factors that affect our results and the accuracy of our model. When the NN has several hidden layers, a linear activation function will simply generate a series of related transformations so, this model is no more expressive than a simple standard logistic regression model. Unless we convey nonlinearity, we are not computing interesting models, even if we delve into neural networks.\n",
        "\n",
        "*Regarding the choice of activation functions in the last layer*, it imposes restrictions on the outputs of the ANN, therefore, its choice depends on the type of problem we want to solve.\n",
        "For example, the sigmoid function only generates positive values ​​between $0$ and $1$, while the hyperbolic tangent provides values ​​between $−1$ and $1$, and the ReLu function only generates positive values ​​between $0$ and $+\\infty$.\n",
        "\n",
        "In our case, we have a regression problem with arbitrary values for output (function adjustment), so, the linear activation function (default in keras) must be used.\n",
        "\n",
        "*Regarding the activation functions of the intermediate layers*, the choice is not so direct. But there are a few tips:\n",
        "* The sigmoid function can be used if both input and output data are in the range $0$ to $1$;\n",
        "* It is a general rule to use the hyperbolic tangent instead of the sigmoid if the input and output data are in the range of $−1$ to $1$;\n",
        "* Hyperbolic and sigmoid are very stable. It is observed, however, that the use of these functions in the intermediate layers increases the difficulty of training the ANN, since in general they cause saturation and vanishing gradients problems (some weights go to zero);\n",
        "* A better choice for the activation function of the intermediate layers is ReLu or leReLu, as they have no saturation problems or small gradients. But they have an opposite problem, gradient explosion (some weights increase a lot). ReLu or leReLu are the most used activation functions for middle layers.\n",
        "\n",
        "Some of these functions are plotted below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsFGHTNtpaMu"
      },
      "source": [
        "x = np.linspace(-3, 3, 500)\n",
        "relu = np.maximum(0, x)\n",
        "sigmoid = 1/(1+np.exp(-x))\n",
        "tanh = np.tanh(x)\n",
        "softplus = np.log(1+np.exp(x))\n",
        "\n",
        "plt.plot(x, relu, label='ReLU',linewidth=1.5)\n",
        "plt.plot(x, sigmoid, label='sigmoid',linewidth=1.5)\n",
        "plt.plot(x, tanh, label='tanh',linewidth=1.5)\n",
        "plt.plot(x, softplus, label='softplus',linewidth=1.5)\n",
        "plt.legend(fancybox=True, framealpha=0.5, loc='upper right')\n",
        "plt.title('Some common activation functions', fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQIG5c0phTF"
      },
      "source": [
        "Now, other activations will be used in the model and its result will be evaluated. The functions that will be used are sigmoid, ReLU, tanh e softplus. Of course, we'll implement in the hidden layer (20 neurons), since the last must have linear activation function because we're implementing a regression problem.\n",
        "\n",
        "Other hyperparameters will be kept constant, with SGD as optimizer, MSE as loss and with (10-20-2) architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcXbHtQipjTy"
      },
      "source": [
        "sigmoid_model = make_model(g = 'sigmoid')\n",
        "relu_model = make_model(g = 'relu')\n",
        "tanh_model = make_model(g = 'tanh')\n",
        "softplus_model = make_model(g = 'softplus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBypJ2NRples"
      },
      "source": [
        "# Training the models\n",
        "print('Sigmoid')\n",
        "sigmoid_history = sigmoid_model.fit(x_train_sca, y_train_sca, epochs=500, batch_size=32, verbose = 0)\n",
        "print('ReLU')\n",
        "relu_history = relu_model.fit(x_train_sca, y_train_sca, epochs=500, batch_size=32, verbose = 0)\n",
        "print('Tanh')\n",
        "tanh_history = tanh_model.fit(x_train_sca, y_train_sca, epochs=500, batch_size=32, verbose = 0)\n",
        "print('Softplus')\n",
        "softplus_history = softplus_model.fit(x_train_sca, y_train_sca, epochs=500, batch_size=32, verbose = 0)\n",
        "print('Done!!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ysGdmYKppJU"
      },
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "plt.title('Training MSE for the models with different activation functions', fontsize=12)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(sigmoid_history.history['loss'], label='sigmoid',linewidth=1.5)\n",
        "plt.plot(relu_history.history['loss'], label='relu',linewidth=1.5)\n",
        "plt.plot(tanh_history.history['loss'], label='tanh',linewidth=1.5)\n",
        "plt.plot(softplus_history.history['loss'], label='softplus',linewidth=1.5)\n",
        "plt.xlim([0,20])\n",
        "plt.legend(fancybox=True, framealpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11NtQda9B1y2"
      },
      "source": [
        "pred_sca_val = sigmoid_model.predict(x_val_sca)\n",
        "y_new_val1 = normalizer_y.inverse_transform(pred_sca_val)\n",
        "\n",
        "pred_sca_val = softplus_model.predict(x_val_sca)\n",
        "y_new_val2 = normalizer_y.inverse_transform(pred_sca_val)\n",
        "\n",
        "Target_vs_Predic2(y_val,y_new_val1,'Sigmoid',y_new_val2, 'Softplus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJr8jVESqQzS"
      },
      "source": [
        "## Diferent Optimizers with different activations\n",
        "A comparison with different optimizers and activation functions will be made. Models with (10-20-18) arquitecture will be made using these diffentent combinations and the results will be gathered and shown.\n",
        "\n",
        "The optimizers that will be used are:\n",
        "+ SGD\n",
        "+ AdaGrad\n",
        "+ Adadelta\n",
        "+ RMSprop\n",
        "+ Adam\n",
        "\n",
        "And the activation functions will be:\n",
        "+ sigmoid\n",
        "+ tanh\n",
        "+ softplus\n",
        "+ ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R69b-_iHqTB5"
      },
      "source": [
        "# Makes a model with different optimizer and activation\n",
        "\n",
        "def make_model_2(activation, optimizer):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(20, activation=activation, input_shape=(10,)))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer=optimizer,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dieth96YqUXs"
      },
      "source": [
        "model_s = {'SGD': {}, 'AdaGrad': {}, 'Adadelta': {}, 'RMSprop': {}, 'Adam': {}}\n",
        "activation_s =  ['sigmoid', 'tanh', 'softplus', 'relu']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEGP9DiLqWNz"
      },
      "source": [
        "i = 0\n",
        "for optimizer in model_s.keys():\n",
        "    for activation in activation_s:\n",
        "        print(f'Combination {i}: {optimizer} with {activation}')\n",
        "        model = make_model_2(activation, optimizer)\n",
        "        hist = model.fit(x_train_sca, y_train_sca, epochs=500, verbose=0)\n",
        "        train_loss = hist.history['loss']\n",
        "        val_loss = model.evaluate(x_val_sca, y_val_sca, verbose=0)\n",
        "        model_s[optimizer][activation] = {'model': model, 'train': train_loss,\n",
        "                                         'val': val_loss, 'hist': hist}\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY_TjbgjqX9R"
      },
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "plt.title('Train MSE for different optimizers\\nusing softplus as activation', fontsize=12)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(model_s['SGD']['softplus']['hist'].history['loss'], label='SGD',linewidth=1.5)\n",
        "plt.plot(model_s['AdaGrad']['softplus']['hist'].history['loss'], label='AdaGrad',linewidth=1.5)\n",
        "plt.plot(model_s['Adadelta']['softplus']['hist'].history['loss'], label='Adadelta',linewidth=1.5)\n",
        "plt.plot(model_s['RMSprop']['softplus']['hist'].history['loss'], label='RMSprop',linewidth=1.5)\n",
        "plt.plot(model_s['Adam']['softplus']['hist'].history['loss'], label='Adam',linewidth=1.5)\n",
        "plt.xlim([0,100])\n",
        "plt.legend(fancybox=True, framealpha=0.5, fontsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsebV_dRqc6Z"
      },
      "source": [
        "Using the softplus as activation function, it can be seen that the training is faster using the SGD or RMSprop. Adam reached also a good result in a slightly larger number of epochs.\n",
        "\n",
        "To show the resulting test and training data in a dataframe, a pandas MultiIndex will be used. To create it, [the user guide from pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) was consulted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkgWI0xnqfjV"
      },
      "source": [
        "arrays = [['SGD', 'AdaGrad', 'Adadelta', 'RMSprop', 'Adam'],\n",
        "          ['Training', 'Val']]\n",
        "index = pd.MultiIndex.from_product(arrays, names=['Optimizer', 'Train/Val'])\n",
        "index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv_ZKlyvqiCV"
      },
      "source": [
        "data = np.zeros((4,10))\n",
        "i = 0\n",
        "for optimizer in model_s.keys():\n",
        "    j = 0\n",
        "    isTest = False\n",
        "    for activation in activation_s:\n",
        "        data[j, i] = model_s[optimizer][activation]['train'][-1]\n",
        "        j += 1\n",
        "    j = 0\n",
        "    i += 1\n",
        "    for activation in activation_s:\n",
        "        data[j, i] = model_s[optimizer][activation]['val'][0]\n",
        "        j += 1\n",
        "    i += 1\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcPEJO9eqjxX"
      },
      "source": [
        "model_s_df = pd.DataFrame(data=data, columns=index, index=activation_s)\n",
        "model_s_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SqAcQ9nqmzv"
      },
      "source": [
        "As can be seen, the best models were obtained with SGD optimizer, with ReLU or softplus as activation.\n",
        "\n",
        "For the next steps, let's initiate with SGD or Adam as optimizers, and ReLU as activation.\n",
        "\n",
        "Before to work adding new layers in our model, let's learn how to save our ANN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3wMby352N3D"
      },
      "source": [
        "## Save a complete ANN\n",
        "\n",
        "To save an ANN developed with Keras to a file in hdf5 format we use the  method `save(file_path_and_name)`. The saved file contains the following information:\n",
        "* ANN architecture;\n",
        "* ANN parameters;\n",
        "* optimizer parameters adopted to train the ANN;\n",
        "* The optimizer status to allow you to continue training exactly where you left off.\n",
        "\n",
        "Then, the method\n",
        "```\n",
        "load_model (file_path_and_name)\n",
        "```\n",
        "is applied to re-establish the ANN as it was when it was saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsNiidse1WHz"
      },
      "source": [
        "# Import library to manipulate files in HDF5 format\n",
        "import h5py\n",
        "\n",
        "# Save the network in the format of an HDF5 dictionary\n",
        "model.save('/content/ANN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zzSZBVx2YlP"
      },
      "source": [
        "### Delete an ANN\n",
        "\n",
        "To delete an ANN from memory, use the `del` method.\n",
        "\n",
        "To load an ANN saved in a file, use the `load_model ()` method.\n",
        "\n",
        "Before using this method you have to load the `load_model` method from the `tensorflow.keras.models` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y_TmHvK2u8t"
      },
      "source": [
        "from keras.models import load_model\n",
        "# Delet the model\n",
        "del model\n",
        "# Recover the ANN from ANN.h5 file\n",
        "model = load_model('/content/ANN.h5')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqKngBRv2v7K"
      },
      "source": [
        "### Save the parameters of an RNA\n",
        "\n",
        "If we want to save only the parameters of an ANN we use the method\n",
        "`save_weigths(file_path_and_name)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrnyZqeW23q0"
      },
      "source": [
        "model.save_weights('/content/ANN_parameters.h5', save_format='h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6wCdAMB24mu"
      },
      "source": [
        "Follow the code below, to load these parameters into another ANN (ann2) with the same architecture used to obtain these parameters,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKF00DAA3BPr"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "# Cria rna2 com mesma arquitetura da rna\n",
        "ann2 = make_model()\n",
        "ann2.summary()\n",
        "# Carrega parâmetros da rede rna na rede rna2\n",
        "ann2.load_weights('/content/ANN_parameters.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_YDVSVcdWhU"
      },
      "source": [
        "There are many situations where we want to develop a new ANN (`ann_new`) using as base another ANN (` ann_basic`) already trained. However, we only want to take advantage of the parameters of some layers of `ann_basic` in `ann_new`. This is possible to do using the methods `save_weights()` and `load_weights()` with small changes. If necessary, consult [here](https://machinelearningmastery.com/save-load-keras-deep-learning-models/), or [here](https://medium.com/swlh/saving-and-loading-of-keras-sequential-and-functional-models-73ce704561f4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI2M1sFipzXt"
      },
      "source": [
        "# <font color=”blue”> Your Homework </font>\n",
        "\n",
        "<font color=”blue”> **Other architectures - Multilayer Neural Networks** </font>\n",
        "\n",
        "\n",
        "<font color=”blue”> Up to now, the models had only one hidden layer. Your homework is divided into two partes: theoretical and practical.\n",
        "\n",
        "<font color=”blue”>__THEORETICAL:__\n",
        "\n",
        "<font color=”blue”> Explain cross validation with your words.\n",
        "\n",
        "<font color=”blue”> Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. So, Adam involves a combination of two gradient descent methodologies: Momentum and RMSProp,\n",
        "\\begin{align}\n",
        "\\begin{split}\n",
        "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) G_t \\\\\n",
        "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) G_t^2\n",
        "\\end{split}\n",
        "\\end{align}\n",
        "\n",
        "<font color=”blue”>$m_t$ and  $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, and $\\beta_1$ $\\beta_2$ are the exponential decay rates for the 1st and 2nd moment estimates, respectively. Default values are, respectively, 0.9 and 0.999.\n",
        "\n",
        "<font color=”blue”> What is the theory behind the ADAM optimizer?\n",
        "\n",
        "\n",
        "<font color=”blue”> __PRACTICAL:__\n",
        "\n",
        "<font color=”blue”> Experiment more hidden layers. Try to improve the performance with your proposed architecture.\n",
        "\n",
        "<font color=”blue”> Use the *cross validation* technique. There is an exercise and explanation aboutt it at the end of this notebook.\n",
        "\n",
        "<font color=”blue”> Discuss your results.</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7bl0dunu8ho"
      },
      "source": [
        "Some important points:\n",
        "* There are not many rules for choosing the number of intermediate layers and the number of neurons in each of the intermediate layers in the first model. The experience and intuition are the main factors that can help in this definition.\n",
        "* In general, the development process starts using a simple ANN with a single intermediate layer. That's what we did.\n",
        "* An indication to define the number of layers and neurons in an ANN is associated with the number of data in the training set. **The the number of parameters in an ANN must be less than the total number of data present in the training data.**\n",
        "* Another important rule to define the neuron number of the intermediate layers is not to create *information bottlenecks*, that is, an intermediate layer can *never* have fewer neurons than the output layer. In a set of layers, each layer has access only to the information in the output of the previous layer. If a layer has few neurons, it will lose some information, which will not be possible to be retrieved in the posterior layers.\n",
        "* The number of neurons in the middle layers depends a lot on the number of inputs and outputs of the training examples and in general we can do some quick initial tests to determine the appropriate minimum number.\n",
        "* Obviously, the number of neurons in the output layer is defined by the number of outputs in the training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w7CihwB6d7j"
      },
      "source": [
        "## Cross validation\n",
        "\n",
        "When the amount of input data is large enough, one can divide the data into *training, validation* and *test* set. The *training set*, as the name suggests, is used to train the model, that is, to adjust the internal parameters of the model such that the inputs match the outputs with the minimum error possible. The *validation set* is used to test the performance of the model before it is subject to the actual test set, and to tune its hyperparameters. The *test set*, which should be kept separate and untouched in the training process, is used to provide an unbiased evaluation of the performance of the final model.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=19txDsk2QD64Y2zjh5c439e8U5KM7j_UZ)\n",
        "\n",
        "\n",
        "When there is no validation set, the most widely used approach is  cross-validation (CV). In the well known K-fold CV the training set is split into $K$ parts - see figure below. The input data set is randomly divided into $K$ subsets (also known as folds). The ML model is trained with $K-1$ subsets, and evaluated in the subset that was not used for training. This process is repeated $K$ times with a different subset reserved for evaluation (and excluded from training) each time. For each training step, the average error is calculated. At the end, the model with the smallest error is selected. This can simulate training/validation, which is useful for hyperparameter tuning, without touching the test set.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=15k96E0QY-QtKpELudeqMMpsApCxibAHJ)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW-kekXooxAk"
      },
      "source": [
        "For illustration, let's apply cross validation for different number of neurons in the intermediate layer (our first model analyzes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83p91hhQowE_"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvoc-7Zzo_HE"
      },
      "source": [
        "def get_cross_val_score(model, x, y, cv=5, epochs=1000):\n",
        "    cvs = np.zeros((cv, len(model.metrics)))\n",
        "\n",
        "    k_folds = KFold(n_splits=cv)\n",
        "    k_folds.split(x, y)\n",
        "    for j, (train_idx, test_idx) in enumerate(k_folds.split(x, y)):\n",
        "        model.fit(x_train_sca[train_idx], y_train_sca[train_idx], epochs=100, verbose=0)\n",
        "        cvs[j,:] = np.array(model.evaluate(x=x_train_sca[test_idx], y=y_train_sca[test_idx], verbose=0))\n",
        "    return cvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPmqySCNpApW"
      },
      "source": [
        "print('10 neurons')\n",
        "cv_10_neurons = get_cross_val_score(model_10_neurons, x_train_sca, y_train_sca, cv=5, epochs=500)\n",
        "print('20 neurons')\n",
        "cv_20_neurons = get_cross_val_score(model_20_neurons, x_train_sca, y_train_sca, cv=5, epochs=500)\n",
        "print('30 neurons')\n",
        "cv_30_neurons = get_cross_val_score(model_30_neurons, x_train_sca, y_train_sca, cv=5, epochs=500)\n",
        "print('40 neurons')\n",
        "cv_40_neurons = get_cross_val_score(model_40_neurons, x_train_sca, y_train_sca, cv=5, epochs=500)\n",
        "print('50 neurons')\n",
        "cv_50_neurons = get_cross_val_score(model_50_neurons, x_train_sca, y_train_sca, cv=5, epochs=500)\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2w8uisFpDJ0"
      },
      "source": [
        "plt.style.use('bmh')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "xi = np.arange(10,60,10)\n",
        "means = np.array([cv_10_neurons[:,0].mean(),\n",
        "                 cv_20_neurons[:,0].mean(),\n",
        "                 cv_30_neurons[:,0].mean(),\n",
        "                 cv_40_neurons[:,0].mean(),\n",
        "                 cv_50_neurons[:,0].mean()])\n",
        "\n",
        "stds  = np.array([cv_10_neurons[:,0].std(),\n",
        "                 cv_20_neurons[:,0].std(),\n",
        "                 cv_30_neurons[:,0].std(),\n",
        "                 cv_40_neurons[:,0].std(),\n",
        "                 cv_50_neurons[:,0].std()])\n",
        "\n",
        "ax.errorbar(xi,\n",
        "            means,\n",
        "            yerr=stds);\n",
        "ax.set(xticks=(xi),\n",
        "       title='MSE\\'s mean plus or minus one standard deviation\\nusing 5-fold cross-validation for different numbers of neurons',\n",
        "       xlabel='Number of neurons',\n",
        "       ylabel='MSE');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8wu7K0epGyY"
      },
      "source": [
        "predictions = np.zeros((5,3))\n",
        "predictions[0,:] = model_10_neurons.evaluate(x_val_sca, y_val_sca)\n",
        "predictions[1,:] = model_20_neurons.evaluate(x_val_sca, y_val_sca)\n",
        "predictions[2,:] = model_30_neurons.evaluate(x_val_sca, y_val_sca)\n",
        "predictions[3,:] = model_40_neurons.evaluate(x_val_sca, y_val_sca)\n",
        "predictions[4,:] = model_50_neurons.evaluate(x_val_sca, y_val_sca)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ASEb1CpIjj"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "xi = np.arange(10,60,10)\n",
        "ax.plot(xi, predictions[:,0])\n",
        "ax.set(xticks=(xi),\n",
        "       title='MSE for the test set\\nfor different numbers of neurons',\n",
        "       xlabel='Number of neurons',\n",
        "       ylabel='MSE');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABHm9QGrynJ7"
      },
      "source": [
        "# Graph of actual and predicted classes\n",
        "def Target_vs_Predic2(y_test,y_pred1,label1,y_pred2,label2):\n",
        "  plt.figure(figsize=(16, 6))\n",
        "  plt.plot(y_test[:,0], 'o', color = 'darkslateblue', label='Target')\n",
        "  plt.plot(y_pred1[:,0], 'o', color = 'crimson', label=label1)\n",
        "  plt.plot(y_pred2[:,0], 'o', color = 'gold', label=label2)\n",
        "  plt.title('Target vs prediction of the ANN')\n",
        "  plt.xlabel('Example')\n",
        "  plt.ylabel(r'$d_{4} [mm]$')\n",
        "  plt.legend()\n",
        "  plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFwlfSqtys0o"
      },
      "source": [
        "pred_sca_val = model_10_neurons.predict(x_val_sca)\n",
        "y_new_val1 = normalizer_y.inverse_transform(pred_sca_val)\n",
        "\n",
        "pred_sca_val = model_40_neurons.predict(x_val_sca)\n",
        "y_new_val2 = normalizer_y.inverse_transform(pred_sca_val)\n",
        "\n",
        "Target_vs_Predic2(y_val,y_new_val1,'10 neurons', y_new_val2, '50 neurons')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}